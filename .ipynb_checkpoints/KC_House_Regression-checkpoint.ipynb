{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring King County, USA Housing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be exploring the King County, USA Housing data set and seeing how well we can get a multivariate linear model to predict the housing prices. The data set can be found [here](https://www.kaggle.com/harlfoxem/housesalesprediction). To tackle this problem we will be using the tensorflow, pandas, and numpy libraries. We are also using sys for outputing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set our seeds and read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: ['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "tf.set_random_seed(7)\n",
    "init_data = pd.read_csv(\"./kc_house_data.csv\")\n",
    "print(\"Col: {0}\".format(list(init_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets look at the data info to see what data types each of these columns are and see if there is any data missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21613 entries, 0 to 21612\n",
      "Data columns (total 21 columns):\n",
      "id               21613 non-null int64\n",
      "date             21613 non-null object\n",
      "price            21613 non-null float64\n",
      "bedrooms         21613 non-null int64\n",
      "bathrooms        21613 non-null float64\n",
      "sqft_living      21613 non-null int64\n",
      "sqft_lot         21613 non-null int64\n",
      "floors           21613 non-null float64\n",
      "waterfront       21613 non-null int64\n",
      "view             21613 non-null int64\n",
      "condition        21613 non-null int64\n",
      "grade            21613 non-null int64\n",
      "sqft_above       21613 non-null int64\n",
      "sqft_basement    21613 non-null int64\n",
      "yr_built         21613 non-null int64\n",
      "yr_renovated     21613 non-null int64\n",
      "zipcode          21613 non-null int64\n",
      "lat              21613 non-null float64\n",
      "long             21613 non-null float64\n",
      "sqft_living15    21613 non-null int64\n",
      "sqft_lot15       21613 non-null int64\n",
      "dtypes: float64(5), int64(15), object(1)\n",
      "memory usage: 3.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(init_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all of the data types are numeric except for \"date\". We can also tell that each column has 21613 rows and none of the columns have missing data. So now we need to get rid of \"id\", because this is useless, and \"date\", because we just want to deal with numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_data = init_data.drop(\"id\", axis=1)\n",
    "init_data = init_data.drop(\"date\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at the how the rest of the data correlates with the \"price\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price            1.000000\n",
      "sqft_living      0.702035\n",
      "grade            0.667434\n",
      "sqft_above       0.605567\n",
      "sqft_living15    0.585379\n",
      "bathrooms        0.525138\n",
      "view             0.397293\n",
      "sqft_basement    0.323816\n",
      "bedrooms         0.308350\n",
      "lat              0.307003\n",
      "waterfront       0.266369\n",
      "floors           0.256794\n",
      "yr_renovated     0.126434\n",
      "sqft_lot         0.089661\n",
      "sqft_lot15       0.082447\n",
      "yr_built         0.054012\n",
      "condition        0.036362\n",
      "long             0.021626\n",
      "zipcode         -0.053203\n",
      "Name: price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "matrix_corr = init_data.corr()\n",
    "print(matrix_corr[\"price\"].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five features under \"price\" seem to show rather strong correlation, with \"zipcode\" being the only negative correlating feature. We might want to experiment with seeing which features are the best to remove but for right now I will just remove \"zipcode\" and test the rest of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_data = init_data.drop(\"zipcode\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define a split function to permutate our data and split it into a training and test set that we can work with. We will have 80% as training data and 20% as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(data, ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "train_set, test_set = split_data(init_data, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split our training set into the data and its labels for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = (train_set.drop(\"price\", axis=1)).values\n",
    "data_labels = (train_set[\"price\"].copy()).values\n",
    "data_labels = data_labels.reshape([len(data_labels),1]) # forcing a [None, 1] shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets pull out some info we need to help set up our model, like the number of features we are using and the number of samples that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = data.shape[1]\n",
    "n_samples = data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to implement our model. We will define our inputs along with the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_init = tf.placeholder(tf.float32, [None, num_features])\n",
    "Y_init = tf.placeholder(tf.float32, [None, 1])\n",
    "W = tf.Variable(tf.random_normal([num_features,1]))\n",
    "b = tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next what we are going to do is standardize our input data so that any possible outliers in the data won't be as big of a problem. Plus this makes the size of the values smaller and easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## calculate mean on the column axis for each column. and I am keeping its deminsions\n",
    "x_mean = tf.reduce_mean(X_init, 0, True)\n",
    "y_mean = tf.reduce_mean(Y_init, 0, True)\n",
    "\n",
    "## Making the input have a mean of 0\n",
    "X_mz = tf.subtract(X_init, x_mean)\n",
    "Y_mz = tf.subtract(Y_init, y_mean)\n",
    "\n",
    "## changing int value to float32 \n",
    "n_samples = tf.constant(n_samples, dtype=tf.float32)\n",
    "\n",
    "x_variance = tf.div(tf.reduce_sum(tf.pow(tf.subtract(X_mz, x_mean), 2), 0, True), tf.subtract(n_samples, 1.0))\n",
    "y_variance = tf.div(tf.reduce_sum(tf.pow(tf.subtract(Y_mz, y_mean), 2), 0, True), tf.subtract(n_samples, 1.0))\n",
    "\n",
    "## Making the input have a variance of 1\n",
    "X = tf.div(X_mz, tf.sqrt(x_variance))\n",
    "Y = tf.div(Y_mz, tf.sqrt(y_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of that taken care of we can define the prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = tf.add(tf.matmul(X,W), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the loss we are going the use the Mean Squared Error(MSE) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pow_val = tf.pow(tf.subtract(pred, Y),2)\n",
    "cost = tf.reduce_mean(pow_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally we would use the gradient descent optimizer to minimize our cost function but the Adam optimizer shows better results. We will also set its learning rate to 0.1, which is kind of high but in this case it converges decently in a short amount of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(1e-1).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets initalize our variables and start our tensorflow session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also define a loss value array to store all of our cost function results to be able to view them graphically later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_values = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define our for loop to iterate through 1000 epochs. We will also make it output our cost results as we are training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 999/1000 cost: 0.09528900682926178\r"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs): \n",
    "    _, c = sess.run([optimizer, cost], feed_dict={X_init:data, Y_init:data_labels})\n",
    "    loss_values.append(c)\n",
    "    sys.stdout.write(\"Epoch: {0}/{1} cost: {2}\\r\".format(epoch+1, num_epochs, c))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to see how our model looks lets print out the final cost, weights, and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done!\n",
      "Final cost: 0.0952889695763588 final weights: [[ -1.75040066e-01]\n",
      " [  1.35566935e-01]\n",
      " [ -1.83122441e-01]\n",
      " [  5.11895679e-03]\n",
      " [  2.11444199e-02]\n",
      " [  8.05988908e-02]\n",
      " [  5.93564287e-02]\n",
      " [  1.67240337e-01]\n",
      " [  1.13748705e+00]\n",
      " [  6.94318235e-01]\n",
      " [  1.67058721e-01]\n",
      " [ -7.53082705e+00]\n",
      " [  1.60125829e-02]\n",
      " [  4.02132072e+01]\n",
      " [ -2.04604931e+01]\n",
      " [  9.88176614e-02]\n",
      " [ -1.64504554e-02]] final biases: [-0.00018118]\n"
     ]
    }
   ],
   "source": [
    "training = sess.run(cost, feed_dict={X_init:data, Y_init:data_labels})\n",
    "print(\"Final cost: {0} final weights: {1} final biases: {2}\".format(training, sess.run(W), sess.run(b)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well we did, we need to compute the $R^2$ to see how well our model explains the data, the Root Mean Squared Error(RMSE) to tell us standard deviation of our data, and the Adjusted $R^2$ function to make sure the regular $R^2$ function is not being influenced by the high number of features we have in our model. The R^2 formula that I am using is $R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Defining R^2\n",
    "ss_e = tf.reduce_sum(tf.pow(tf.subtract(Y, pred), 2))\n",
    "ss_t = tf.reduce_sum(tf.pow(tf.subtract(Y, 0), 2))\n",
    "r2 = tf.subtract(1.0, tf.div(ss_e, ss_t))\n",
    "\n",
    "## Defining Adjusted R^2\n",
    "adjusted_r2 = tf.subtract(1.0, tf.div(tf.div(ss_e, (n_samples - 1.0)), tf.div(ss_t, (n_samples - num_features - 1)) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we grab all of the predictions and all of the standardized Y values. Then we compute the $R^2$, Adjusted $R^2$, and RMSE on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse of pred_data and std_y_data is: 0.3086891174316406\n",
      "R^2 value: 0.6991617679595947\n",
      "Adjusted R^2 value: 0.6994575262069702\n"
     ]
    }
   ],
   "source": [
    "pred_data = sess.run(pred, feed_dict={X_init:data, Y_init:data_labels})\n",
    "std_y_data = sess.run(Y, feed_dict={Y_init:data_labels})\n",
    "## computing rmse\n",
    "rmse = np.sqrt(np.mean(np.power(np.subtract(pred_data, std_y_data), 2)))\n",
    "print(\"rmse of pred_data and std_y_data is: {0}\".format(rmse))\n",
    "print(\"R^2 value: {0}\".format(sess.run(r2,feed_dict={X_init:data, Y_init:data_labels})) )\n",
    "print(\"Adjusted R^2 value: {0}\".format(sess.run(adjusted_r2, feed_dict={X_init:data, Y_init:data_labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have a RMSE of ~0.30, $R^2$ of ~70, and Adjusted $R^2$ of ~70 with our simple multivariate linear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
